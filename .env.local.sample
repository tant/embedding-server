# .env.local.sample
# Sample environment file for embedding-server.
# Copy this file to `.env.local` and edit values as needed.
# NOTE: `.env.local` is intended to be local-only and gitignored. Do NOT commit secrets.

# --- Server settings ---------------------------------------------------------
# Port uvicorn will listen on when launched via `uv` or exported as PORT.
# Default uvicorn port is 8000; this is an internal application port variable used by the
# app; uv/uvicorn looks at the shell PORT environment when starting the server.
# Example: export PORT=8000
PORT=8000



# Host/interface (usually 0.0.0.0 for containers) — uvicorn flag when launching.
# Not required by the app itself, but useful reference.
HOST=0.0.0.0

# Number of worker processes. For GPU deployments, keep this at 1 to avoid multiple
# processes loading the same GPU model. For CPU-only, you may increase based on cores.
WORKERS=1

# --- Embedding app settings --------------------------------------------------
# Device for embedding model: 'cpu' or 'cuda' (or 'auto' if you implement detection).
# Use 'cpu' by default to avoid GPU VRAM issues unless you intentionally run with CUDA.
EMBED_DEVICE=cpu

# Optional app metadata (customize API title/description/version shown in docs)
EMBED_APP_TITLE=Embedding API
EMBED_APP_DESC=API to generate sentence embeddings using Sentence-Transformers.
EMBED_APP_VERSION=1.1.0

# Model identifier (HuggingFace format). Change to any sentence-transformers or HF model
# that is compatible with sentence-transformers and your hardware.
EMBED_MODEL_NAME=BAAI/bge-m3-unsupervised

# Path to a local model cache directory (optional). If set, HF will use this directory
# to read/write model files (helps in offline or containerized setups).
# Example: /var/cache/huggingface
EMBED_MODEL_CACHE=./model-cache

# Local HuggingFace model cache directory. The application will map this to HF_HOME so
# HuggingFace tools use this location. Avoid using TRANSFORMERS_CACHE (deprecated).

# HuggingFace authentication token (optional). Use for private models or higher rate limits.
# You can set either HF_TOKEN or HUGGINGFACE_HUB_TOKEN; the app reads HF_TOKEN and will export
# it as HUGGINGFACE_HUB_TOKEN before loading models.
HF_TOKEN=
# Example: HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
HUGGINGFACE_HUB_TOKEN=
# Example: HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# If set to 1, skip loading the model at startup (useful for CI or fast health checks).
# The server will start but `ready` may be false until a model is loaded manually.
EMBED_SKIP_MODEL_LOAD=0

# Maximum number of items accepted per request. Server validates this to prevent OOM.
EMBED_MAX_ITEMS=128

# Maximum length (characters/tokens) per input text. Enforced to avoid extremely
# long inputs that cause memory blowups. Adjust per model/tokenizer.
EMBED_MAX_TEXT_LEN=2048

# Batch size used when encoding texts. Tune for latency vs memory usage.
EMBED_BATCH_SIZE=32

# CORS origins (comma-separated or *). Use specific origins in production.
EMBED_CORS_ORIGINS=*

# --- Logging -----------------------------------------------------------------
# Path to log file. The app's default logging config already writes to `embedding-server.log`.
LOG_FILE=embedding-server.log

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# --- Optional / Advanced ----------------------------------------------------
# Timeouts and worker tuning (example values; tweak per deployment)
REQUEST_TIMEOUT_SECONDS=30
MODEL_LOAD_TIMEOUT_SECONDS=120

# Telemetry / monitoring endpoints (optional) — leave empty if not used
PROMETHEUS_ENDPOINT=

# --- Usage notes ------------------------------------------------------------
# After copying this file to `.env.local` you can load the variables into your shell:
#   export $(grep -v '^#' .env.local | xargs)
# Or use a tool like `direnv`, `python-dotenv`, or the `uv` runner which auto-loads `.env.local`.

# Run with uv (recommended if you use the `uv` tool):
#   uv run uvicorn main:app --host $HOST --port $PORT --workers $WORKERS

# Or run uvicorn directly (after exporting .env.local):
#   uvicorn main:app --host 0.0.0.0 --port $PORT --workers $WORKERS

# Security reminders:
# - Do not store secrets, tokens, or credentials in .env.local if it will be committed.
# - Use environment-specific secret stores (vaults, cloud secret managers) when possible.
# - Restrict `EMBED_CORS_ORIGINS` in production to trusted domains.

# Quick tips:
# - For GPU: set EMBED_DEVICE=cuda and WORKERS=1. Ensure your torch build supports CUDA.
# - For CI/tests: set EMBED_SKIP_MODEL_LOAD=1 to start the app without downloading models.
# - To pin a model snapshot, you can set EMBED_MODEL_CACHE to a folder containing a
#   pre-downloaded HF snapshot and configure EMBED_MODEL_NAME accordingly.

# End of sample
